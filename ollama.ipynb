{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-ollama\n",
      "  Obtaining dependency information for langchain-ollama from https://files.pythonhosted.org/packages/85/72/f7301340a544fea1c139c64590044f0beb5bfba889b8f6e50766b933e660/langchain_ollama-0.2.3-py3-none-any.whl.metadata\n",
      "  Downloading langchain_ollama-0.2.3-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.33 in c:\\users\\david\\anaconda3\\lib\\site-packages (from langchain-ollama) (0.3.34)\n",
      "Collecting ollama<1,>=0.4.4 (from langchain-ollama)\n",
      "  Obtaining dependency information for ollama<1,>=0.4.4 from https://files.pythonhosted.org/packages/31/83/c3ffac86906c10184c88c2e916460806b072a2cfe34cdcaf3a0c0e836d39/ollama-0.4.7-py3-none-any.whl.metadata\n",
      "  Downloading ollama-0.4.7-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\david\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-ollama) (0.1.147)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\david\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-ollama) (8.2.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\david\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-ollama) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\david\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-ollama) (6.0)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\david\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-ollama) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\david\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-ollama) (4.12.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in c:\\users\\david\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-ollama) (2.9.1)\n",
      "Requirement already satisfied: httpx<0.29,>=0.27 in c:\\users\\david\\anaconda3\\lib\\site-packages (from ollama<1,>=0.4.4->langchain-ollama) (0.27.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\david\\anaconda3\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (3.5.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\david\\anaconda3\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\david\\anaconda3\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\users\\david\\anaconda3\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (3.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\david\\anaconda3\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\david\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\david\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (2.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\david\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (3.10.7)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\david\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\david\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\david\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.3 in c:\\users\\david\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (2.23.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\david\\anaconda3\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\david\\anaconda3\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (1.26.19)\n",
      "Downloading langchain_ollama-0.2.3-py3-none-any.whl (19 kB)\n",
      "Downloading ollama-0.4.7-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: ollama, langchain-ollama\n",
      "Successfully installed langchain-ollama-0.2.3 ollama-0.4.7\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an obedient assistant that fulfils text-based requests and answers questions.\n",
      "You always use the first person or impersonal tone and never use the second person in your responses.\n",
      "1. If available, use the context to answer the question at the end.\n",
      "2. If you don't know the answer, just say that \"I don't know\" but don't make up an answer on your own.\n",
      "3. Keep the answer concise and specific.\n",
      "Context: NA\n",
      "Prompt: What is the capital of France?\n",
      "Answer:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Thinking... Okay, so I need to figure out the capital of France. Hmm, let me think about this. I remember\n",
      "learning in school that capitals are usually names or places. France has a few major cities. I'm\n",
      "pretty sure Paris is one of them because it's a big city and often comes up when people talk about\n",
      "France. But wait, what about other cities? Like maybe another city named Paris? No, no, Paris is the\n",
      "capital. There's also mentions of other capitals in French-speaking countries, but since this is\n",
      "specifically about France, I should focus on that.  I think Paris has been the capital since at\n",
      "least the 18th century or so. Yeah, I'm pretty sure it's been around for a long time now. The rest\n",
      "of France isn't the capital; cities like Cr√©cy or Saint-Quentin are more important in local\n",
      "contexts, maybe. But Paris is definitely France's capital.  So putting that together, the answer\n",
      "should be Paris. That seems right because I've heard people mention Paris being the heartland and a\n",
      "central location within France.\n",
      "\n",
      " Response:\n",
      " Paris.\n"
     ]
    }
   ],
   "source": [
    "llm = OllamaLLM(model=\"deepseek-r1:1.5b\")\n",
    "\n",
    "# Define the prompt\n",
    "pre_prompt = \"\"\"\n",
    "You are an obedient assistant that fulfills text-based requests and answers questions.\n",
    "You always use the first person or impersonal tone and never use the second person in your responses.\n",
    "1. If available, use the context to answer the question at the end.\n",
    "2. If you don't know the answer, just say that \"I don't know\" but don't make up an answer on your own.\n",
    "3. Keep the answer concise and specific.\n",
    "Context: {context}\n",
    "Prompt: {prompt}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(pre_prompt)\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=QA_CHAIN_PROMPT,\n",
    "    callbacks=None,\n",
    "    verbose=True)\n",
    "\n",
    "context = \"NA\"\n",
    "prompt = \"What is the capital of France?\"\n",
    "response = llm_chain.run(context=context, prompt=prompt)\n",
    "\n",
    "think_part = response[response.find(\"<think>\")+7:response.find(\"</think>\")].strip()\n",
    "answer_part = response[response.find(\"</think>\")+8:].strip()\n",
    "\n",
    "wrapper = textwrap.TextWrapper(width=100)\n",
    "wrapped_thought = wrapper.fill(think_part)\n",
    "wrapped_response = wrapper.fill(answer_part)\n",
    "print(f\"Thinking... {wrapped_thought}\\n\\n Response:\\n {wrapped_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!streamlit run deepseek_chat.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
