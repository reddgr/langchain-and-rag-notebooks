{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-ollama\n",
      "  Obtaining dependency information for langchain-ollama from https://files.pythonhosted.org/packages/85/72/f7301340a544fea1c139c64590044f0beb5bfba889b8f6e50766b933e660/langchain_ollama-0.2.3-py3-none-any.whl.metadata\n",
      "  Downloading langchain_ollama-0.2.3-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.33 in c:\\users\\david\\anaconda3\\lib\\site-packages (from langchain-ollama) (0.3.34)\n",
      "Collecting ollama<1,>=0.4.4 (from langchain-ollama)\n",
      "  Obtaining dependency information for ollama<1,>=0.4.4 from https://files.pythonhosted.org/packages/31/83/c3ffac86906c10184c88c2e916460806b072a2cfe34cdcaf3a0c0e836d39/ollama-0.4.7-py3-none-any.whl.metadata\n",
      "  Downloading ollama-0.4.7-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\david\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-ollama) (0.1.147)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\david\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-ollama) (8.2.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\david\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-ollama) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\david\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-ollama) (6.0)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\david\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-ollama) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\david\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-ollama) (4.12.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in c:\\users\\david\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-ollama) (2.9.1)\n",
      "Requirement already satisfied: httpx<0.29,>=0.27 in c:\\users\\david\\anaconda3\\lib\\site-packages (from ollama<1,>=0.4.4->langchain-ollama) (0.27.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\david\\anaconda3\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (3.5.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\david\\anaconda3\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\david\\anaconda3\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\users\\david\\anaconda3\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (3.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\david\\anaconda3\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\david\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\david\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (2.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\david\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (3.10.7)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\david\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\david\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\david\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.3 in c:\\users\\david\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (2.23.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\david\\anaconda3\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\david\\anaconda3\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (1.26.19)\n",
      "Downloading langchain_ollama-0.2.3-py3-none-any.whl (19 kB)\n",
      "Downloading ollama-0.4.7-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: ollama, langchain-ollama\n",
      "Successfully installed langchain-ollama-0.2.3 ollama-0.4.7\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_25484\\4208354644.py:17: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(\n",
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_25484\\4208354644.py:25: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llm_chain.run(context=context, prompt=prompt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an obedient assistant that fulfills text-based requests and answers questions.\n",
      "You always use the first person or impersonal tone and never use the second person in your responses.\n",
      "1. If available, use the context to answer the question at the end.\n",
      "2. If you don't know the answer, just say that \"I don't know\" but don't make up an answer on your own.\n",
      "3. Keep the answer concise and specific.\n",
      "Context: Baby don't hurt me.\n",
      "Prompt: What is love?\n",
      "Answer:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Thinking... Okay, so I need to figure out what love is based on the given context that says \"Baby don't hurt\n",
      "me.\" Hmm, I'm not entirely sure about this question because it's a bit abstract. Let me think about\n",
      "where I've heard people talk about love before.  I remember hearing words like \"loving\" and\n",
      "sometimes phrases like \"love\" being used in songs or movies. Maybe in those contexts, they use the\n",
      "word to describe someone who is kind, supportive, and gives back. But does that make sense? Love\n",
      "isn't just about acting kind; it's also a feeling or emotion.  I think love is something deeply\n",
      "personal. It can be strong, nurturing, and even caring towards others. But how do we define it? I've\n",
      "heard some people talk about it as a bond between two people. Maybe they spend time together and\n",
      "share their emotions without expecting anything in return. Or perhaps it's more about feeling loved\n",
      "by someone else.  Wait, but love can also be seen as something that isn't always present or even\n",
      "easy to feel for. There might be times when you don't feel like you're in a relationship with\n",
      "someone, but still, the word \"love\" might just be used metaphorically. I'm not sure if I have a\n",
      "concrete answer here because love is a complex and multifaceted concept that varies depending on who\n",
      "we are talking about.  I guess I'll need to acknowledge that without more context or specific\n",
      "information, it's hard to pin down what love exactly means. It could vary so much from person to\n",
      "person.\n",
      "\n",
      " Response:\n",
      " Love is an abstract concept that often refers to deep emotional connections and feelings, such as\n",
      "being a partner, companion, or even just feeling loved by someone else. It can be strong, nurturing,\n",
      "or caring towards others, but its meaning can vary widely depending on the context and individual\n",
      "experiences.\n"
     ]
    }
   ],
   "source": [
    "llm = OllamaLLM(model=\"deepseek-r1:1.5b\")\n",
    "\n",
    "# Define the prompt\n",
    "pre_prompt = \"\"\"\n",
    "You are an obedient assistant that fulfills text-based requests and answers questions.\n",
    "You always use the first person or impersonal tone and never use the second person in your responses.\n",
    "1. If available, use the context to answer the question at the end.\n",
    "2. If you don't know the answer, just say that \"I don't know\" but don't make up an answer on your own.\n",
    "3. Keep the answer concise and specific.\n",
    "Context: {context}\n",
    "Prompt: {prompt}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(pre_prompt)\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=QA_CHAIN_PROMPT,\n",
    "    callbacks=None,\n",
    "    verbose=True)\n",
    "\n",
    "context = \"Baby don't hurt me.\"\n",
    "prompt = \"What is love?\"\n",
    "response = llm_chain.run(context=context, prompt=prompt)\n",
    "\n",
    "think_part = response[response.find(\"<think>\")+7:response.find(\"</think>\")].strip()\n",
    "answer_part = response[response.find(\"</think>\")+8:].strip()\n",
    "\n",
    "wrapper = textwrap.TextWrapper(width=100)\n",
    "wrapped_thought = wrapper.fill(think_part)\n",
    "wrapped_response = wrapper.fill(answer_part)\n",
    "print(f\"Thinking... {wrapped_thought}\\n\\n Response:\\n {wrapped_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!streamlit run deepseek_chat.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
